{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package abc to\n",
      "[nltk_data]   Package abc is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from language_model import LM\n",
    "import nltk.corpus\n",
    "nltk.download('abc')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766811"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nltk.corpus.abc.sents()\n",
    "len([word for sent in sentences for word in sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to create an instance of the model. if n_type is equal to 'bigram', then an instance of a bigram model is created. If n_type is set to trigram, then a 'trigram' model is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_model = LM(n_type='bigram')\n",
    "tri_model = LM(n_type='trigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to train the model. The input must be a list of lists, where each list is a sentence. All the cleaning and padding is handled by the model, so we just have to feed it with raw sentences (for example, for the bigram model, if we pass to the train method the sentence ['ThI,!s', 'iS', '.A.', 'tE==++sT'], the model will transform the sentence to ['* start *', 'this', 'is', 'a', 'test', '* end *'] and it will create all the necessary vocabularies.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, then the following methods can be called:\n",
    "Let b_model be an instance of a bigram model:\n",
    " - b_model.unigram_voc\_ -> this will return the vocabulary of all unigrams\n",
    " - b_model.bigram_voc_ -> this will return the vocabulary of all bigrams (for the trigram model we would have to make an instance of a trigram model using LM(n_type='trigram'))\n",
    " - b_model.add_a_prob() -> this will calculate the probability of a given bigram (or trigram if we use the trigram model) using Add-a smoothing, where the hyper-parameter 'a' can be tuned in order to achieve the lowest possible Cross-Entropy\n",
    " - b_model.kn_prob() -> this will calculate the probability of a given bigram using the interpolated Kneser-Ney smoothing, where the constant D is equal to 0.5 for bigrams with a count of 1 and 0.75 for the rest.\n",
    " - b_model.estimate_sent_prob() -> this will calculate the log probabilities of all the given sentences. If more than one sentence is given as an input, then it will return a list with the probabilities of each sentence (which could then be summed and thus, calculate the total log probability of ,eg, the test corpus). There are two available smoothers for bigrams. Add-a smoothing and Interpolated Kneser-Ney smoothing.\n",
    " - b_model.entr_perp() -> this will return the Cross-Entropy and Perplexity of the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other static methods can be called (they don't require an instance creation):\n",
    " - Model.word_cleaner() -> this will clean any list of words. It will remove any character that is not a letter or number, and it will apply lower case to all the words\n",
    " - Model.sent_preprocessing() -> this will clean and pad any sentence that is given as an input\n",
    " - Model.bigram() -> this will create bigrams of a given sentence\n",
    " - Model.trigram() -> this will create trigrams of a given sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_model.train(sentences)  # train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating the probability of a bigram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add-a smoothing (with a=1) probability: 0.04117362955807776\n",
      "K-N probability: 0.12414558851175722\n"
     ]
    }
   ],
   "source": [
    "laplace = bi_model.add_a_prob(('this', 'is'))\n",
    "kn = bi_model.kn_prob(('this', 'is'))\n",
    "print(f\"Add-a smoothing (with a=1) probability: {laplace}\\nK-N probability: {kn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating the log probability of a sentence using laplace smoothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-34.326750073789114]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_model.estimate_sent_prob([['this', 'is', 'a', 'test']], smoothing='add_a')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating the log probability of a sentence using the Interpolated Kneser-Ney smoother:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-26.54752395643943]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_model.estimate_sent_prob([['this', 'is', 'a', 'test']], smoothing='kn')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the bigram vocabulary attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('*start*', 'pm'): 9,\n",
       "         ('pm', 'denies'): 1,\n",
       "         ('denies', 'knowledge'): 1,\n",
       "         ('knowledge', 'of'): 18,\n",
       "         ('of', 'awb'): 36,\n",
       "         ('awb', 'kickbacks'): 5,\n",
       "         ('kickbacks', 'the'): 1,\n",
       "         ('the', 'prime'): 40,\n",
       "         ('prime', 'minister'): 91,\n",
       "         ('minister', 'has'): 5,\n",
       "         ('has', 'denied'): 7,\n",
       "         ('denied', 'he'): 2,\n",
       "         ('he', 'knew'): 4,\n",
       "         ('knew', 'awb'): 2,\n",
       "         ('awb', 'was'): 9,\n",
       "         ('was', 'paying'): 3,\n",
       "         ('paying', 'kickbacks'): 2,\n",
       "         ('kickbacks', 'to'): 12,\n",
       "         ('to', 'iraq'): 32,\n",
       "         ('iraq', 'despite'): 1,\n",
       "         ('despite', 'writing'): 1,\n",
       "         ('writing', 'to'): 2,\n",
       "         ('to', 'the'): 1469,\n",
       "         ('the', 'wheat'): 75,\n",
       "         ('wheat', 'exporter'): 49,\n",
       "         ('exporter', 'asking'): 1,\n",
       "         ('asking', 'to'): 1,\n",
       "         ('to', 'be'): 1215,\n",
       "         ('be', 'kept'): 7,\n",
       "         ('kept', 'fully'): 1,\n",
       "         ('fully', 'informed'): 1,\n",
       "         ('informed', 'on'): 1,\n",
       "         ('on', 'iraq'): 4,\n",
       "         ('iraq', 'wheat'): 11,\n",
       "         ('wheat', 'sales'): 8,\n",
       "         ('sales', '*end*'): 15,\n",
       "         ('*start*', 'letters'): 1,\n",
       "         ('letters', 'from'): 1,\n",
       "         ('from', 'john'): 2,\n",
       "         ('john', 'howard'): 35,\n",
       "         ('howard', 'and'): 3,\n",
       "         ('and', 'deputy'): 3,\n",
       "         ('deputy', 'prime'): 22,\n",
       "         ('minister', 'mark'): 24,\n",
       "         ('mark', 'vaile'): 37,\n",
       "         ('vaile', 'to'): 2,\n",
       "         ('to', 'awb'): 9,\n",
       "         ('awb', 'have'): 3,\n",
       "         ('have', 'been'): 747,\n",
       "         ('been', 'released'): 9,\n",
       "         ('released', 'by'): 12,\n",
       "         ('by', 'the'): 607,\n",
       "         ('the', 'cole'): 85,\n",
       "         ('cole', 'inquiry'): 88,\n",
       "         ('inquiry', 'into'): 31,\n",
       "         ('into', 'the'): 372,\n",
       "         ('the', 'oil'): 84,\n",
       "         ('oil', 'for'): 77,\n",
       "         ('for', 'food'): 98,\n",
       "         ('food', 'program'): 17,\n",
       "         ('program', '*end*'): 34,\n",
       "         ('*start*', 'in'): 579,\n",
       "         ('in', 'one'): 56,\n",
       "         ('one', 'of'): 452,\n",
       "         ('of', 'the'): 4666,\n",
       "         ('the', 'letters'): 3,\n",
       "         ('letters', 'mr'): 1,\n",
       "         ('mr', 'howard'): 19,\n",
       "         ('howard', 'asks'): 1,\n",
       "         ('asks', 'awb'): 1,\n",
       "         ('awb', 'managing'): 3,\n",
       "         ('managing', 'director'): 63,\n",
       "         ('director', 'andrew'): 9,\n",
       "         ('andrew', 'lindberg'): 12,\n",
       "         ('lindberg', 'to'): 1,\n",
       "         ('to', 'remain'): 12,\n",
       "         ...})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_model.bigram_voc_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything that was presented can also be done for a trigram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proving that the models work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sents, test_set, _, _ = train_test_split(sentences, sentences, test_size=0.2, random_state=42)  # keep test set \n",
    "train_set, dev_set, _, _ = train_test_split(train_sents, train_sents, test_size=0.1, random_state=42)  # split the train set to dev and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_model = LM(n_type='bigram')\n",
    "bi_model.train(train_set)\n",
    "\n",
    "tri_model = LM(n_type='trigram')\n",
    "tri_model.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy and Perplexity using the Bigram Model with Add-a Smoothing: 6.780454239756531 and 109.93098273596117\n",
      "Cross-Entropy and Perplexity using the Bigram Model with K-N Smoothing: 6.427927593043861 and 86.09918003297753\n",
      "Cross-Entropy and Perplexity using the Trigram Model with Add-a Smoothing: 8.560095402262942 and 377.43787794141315\n"
     ]
    }
   ],
   "source": [
    "lap_hc, lap_pp = bi_model.entr_perp(test_set, a=0.01)\n",
    "kn_hc, kn_pp = bi_model.entr_perp(test_set, smoothing='kn')\n",
    "tri_hc, tri_pp = tri_model.entr_perp(test_set, a=0.007)\n",
    "\n",
    "print(f\"Cross-Entropy and Perplexity using the Bigram Model with Add-a Smoothing: {lap_hc} and {lap_pp}\")\n",
    "print(f\"Cross-Entropy and Perplexity using the Bigram Model with K-N Smoothing: {kn_hc} and {kn_pp}\")\n",
    "print(f\"Cross-Entropy and Perplexity using the Trigram Model with Add-a Smoothing: {tri_hc} and {tri_pp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "test_shfl = test_set[:]\n",
    "foo = [shuffle(sent) for sent in test_shfl]  # shuffle the order of words from each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy and Perplexity using the Bigram Model with Add-a Smoothing on shuffled sentences: 9.356070591143954 and 655.3267402755667\n",
      "Cross-Entropy and Perplexity using the Bigram Model with K-N Smoothing on shuffled sentences: 8.231113955160426 and 300.4776667419467\n",
      "Cross-Entropy and Perplexity using the Trigram Model with Add-a Smoothing on shuffled sentences: 10.727254898805462 and 1695.2177679941951\n"
     ]
    }
   ],
   "source": [
    "lap_hc, lap_pp = bi_model.entr_perp(test_shfl, a=0.01)\n",
    "kn_hc, kn_pp = bi_model.entr_perp(test_shfl, smoothing='kn')\n",
    "tri_hc, tri_pp = tri_model.entr_perp(test_shfl, a=0.007)\n",
    "\n",
    "print(f\"Cross-Entropy and Perplexity using the Bigram Model with Add-a Smoothing on shuffled sentences: {lap_hc} and {lap_pp}\")\n",
    "print(f\"Cross-Entropy and Perplexity using the Bigram Model with K-N Smoothing on shuffled sentences: {kn_hc} and {kn_pp}\")\n",
    "print(f\"Cross-Entropy and Perplexity using the Trigram Model with Add-a Smoothing on shuffled sentences: {tri_hc} and {tri_pp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models seems to assign lower probabilities (higher cross-entropy and perplexity) to ‘non-sense’ sentences, which means that the models are working!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the most probable next word using the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def next_word(word, model, smoother='kn', a=1):  # This function will return the top 10 most probable word continuations\n",
    "    voc = list(model.unigram_voc_.keys())\n",
    "    voc.remove('*start*')\n",
    "    voc.remove('*UNK*')\n",
    "    voc.remove('*end*')\n",
    "    \n",
    "    if smoother == 'kn':\n",
    "        next_word = {key: bi_model.kn_prob((word, key)) for key in voc}\n",
    "    else:\n",
    "        next_word = {key: bi_model.estimate_ngram_prob((word, key), a=a) for key in voc}\n",
    "\n",
    "    sorted_words = dict(sorted(next_word.items(), key=lambda item: item[1]))\n",
    "    top10 = {i: sorted_words[i] for i in list(sorted_words.keys())[-10:]}\n",
    "\n",
    "    return pd.DataFrame(top10.items()).rename(columns = {0: word, 1: 'Probability'}).sort_values(by='Probability', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>he</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>said</td>\n",
       "      <td>0.393450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>says</td>\n",
       "      <td>0.334239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is</td>\n",
       "      <td>0.042016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>has</td>\n",
       "      <td>0.019871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>was</td>\n",
       "      <td>0.014306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>will</td>\n",
       "      <td>0.012982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>had</td>\n",
       "      <td>0.008395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>0.007996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>also</td>\n",
       "      <td>0.007358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adds</td>\n",
       "      <td>0.007307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     he  Probability\n",
       "9  said     0.393450\n",
       "8  says     0.334239\n",
       "7    is     0.042016\n",
       "6   has     0.019871\n",
       "5   was     0.014306\n",
       "4  will     0.012982\n",
       "3   had     0.008395\n",
       "2   and     0.007996\n",
       "1  also     0.007358\n",
       "0  adds     0.007307"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_word('he', bi_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>news</td>\n",
       "      <td>0.097392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>for</td>\n",
       "      <td>0.032394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>at</td>\n",
       "      <td>0.027989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>to</td>\n",
       "      <td>0.027153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td>0.024799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as</td>\n",
       "      <td>0.017111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prices</td>\n",
       "      <td>0.015528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>enough</td>\n",
       "      <td>0.015374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thing</td>\n",
       "      <td>0.015267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>on</td>\n",
       "      <td>0.013858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     good  Probability\n",
       "9    news     0.097392\n",
       "8     for     0.032394\n",
       "7      at     0.027989\n",
       "6      to     0.027153\n",
       "5     and     0.024799\n",
       "4      as     0.017111\n",
       "3  prices     0.015528\n",
       "2  enough     0.015374\n",
       "1   thing     0.015267\n",
       "0      on     0.013858"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_word('good', bi_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a</td>\n",
       "      <td>0.126892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the</td>\n",
       "      <td>0.121113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>it</td>\n",
       "      <td>0.114721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sure</td>\n",
       "      <td>0.077509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>up</td>\n",
       "      <td>0.046269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>them</td>\n",
       "      <td>0.030285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sense</td>\n",
       "      <td>0.016443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>any</td>\n",
       "      <td>0.014327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>an</td>\n",
       "      <td>0.012478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people</td>\n",
       "      <td>0.012267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     make  Probability\n",
       "9       a     0.126892\n",
       "8     the     0.121113\n",
       "7      it     0.114721\n",
       "6    sure     0.077509\n",
       "5      up     0.046269\n",
       "4    them     0.030285\n",
       "3   sense     0.016443\n",
       "2     any     0.014327\n",
       "1      an     0.012478\n",
       "0  people     0.012267"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_word('make', bi_model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d730a2b9894ef36cca77d717dfbf37657b5f05c487fae01f16d101a9a41534c0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
